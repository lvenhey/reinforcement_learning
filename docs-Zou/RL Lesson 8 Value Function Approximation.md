## RL Lesson 8: Value Function Approximation

What is value function approximation?

It uses linear equation or second-order curver and so on to approximate the state value with a given policy.

Then if the result is accurate enough, then the estimated state value can be used to calculate the action value and so to do the policy improvement step.

The approximation equation's parameters are denoted as parameter vector namely the $\omega$.

Our goal is to find an optimal $\omega$ to minimize the differences bewteen $\hat{v}(S,\omega)$ and $v_{\pi}(s)$ 

The object function:

$$
J(\omega) = E[(v_{\pi}(s) - \hat{v}(S,\omega))^2]
$$

So how to calculate the Expectation? We need to define the probability distribution of S.

* The first way is using the uniform distribution.
* The second way is to use stationary distribution.
  It describes a long-run behavior of a Markov-process.

$$
J(\omega) = \sum_{s \in S} d_{\pi}(s) (v_{\pi}(s) - \hat{v}(s,\omega))^2
$$

where $d_{\pi}$ is the probabilty.

Let $n_{\pi}(s)$ be the number of times that s has been visited in a very long episode generated by $\pi$

Then,

$$
d_{\pi}(s) \approx \frac{n_{\pi}(s)}{\sum_{s^{ '} \in S} n_{\pi}(s^{ '})}
$$

Meanwhile , $d_{\pi}$ can be calculated by

$$
d_{\pi}^T = d_{\pi}^T P_{\pi}
$$

where $P_{\pi}$ is from the Bellman Equation with matrix form.



