## RL Lesson 8: Value Function Approximation

What is value function approximation?

It uses linear equation or second-order curver and so on to approximate the state value with a given policy.

Then if the result is accurate enough, then the estimated state value can be used to calculate the action value and so to do the policy improvement step.

The approximation equation's parameters are denoted as parameter vector namely the $\omega$.

Our goal is to find an optimal $\omega$ to minimize the differences bewteen $\hat{v}(S,\omega)$ and $v_{\pi}(s)$ 

The object function:

$$
J(\omega) = E[(v_{\pi}(s) - \hat{v}(S,\omega))^2]
$$

So how to calculate the Expectation? We need to define the probability distribution of S.

* The first way is using the uniform distribution.
* The second way is to use stationary distribution.
  It describes a long-run behavior of a Markov-process.

$$
J(\omega) = \sum_{s \in S} d_{\pi}(s) (v_{\pi}(s) - \hat{v}(s,\omega))^2
$$

where $d_{\pi}$ is the probabilty.

Let $n_{\pi}(s)$ be the number of times that s has been visited in a very long episode generated by $\pi$

Then,

$$
d_{\pi}(s) \approx \frac{n_{\pi}(s)}{\sum_{s^{ '} \in S} n_{\pi}(s^{ '})}
$$

Meanwhile , $d_{\pi}$ can be calculated by

$$
d_{\pi}^T = d_{\pi}^T P_{\pi}
$$

where $P_{\pi}$ is from the Bellman Equation with matrix form.
____
### How to minimize the objective function $J(\omega)$?
1、The gradient-descent algorithm

$$
\omega_{k+1} = \omega_k - \alpha \nabla_{\omega}J(\omega_k)
$$

Note that $\omega$ is the parameter vecor used to calculate the objective function.

2、Using stochastic gradient descent and TD algorithm

$$
\omega_{t+1} = \omega_t + \alpha_t [r_{t+1} + \gamma \hat{v}(s_{t+1},\omega_t -\hat{v}(s_t,\omega_t)] \nabla_{\omega} \hat{v}(s_t,\omega_t)
$$

3、How to select the function $\hat{v}(s,\omega)$ ?

Usually use the linear form $\hat{v}(s,\omega) = \Phi^T(s) \omega $ 

Then the algorithm becomes:

$$
\omega_{t+1} = \omega_t + \alpha_t [r_{t+1} + \gamma \Phi^T(s_{t+1}) \omega_t - \Phi^T(s_t)\omega_t] \Phi(s_t)
$$



_____
### Deep Q-learning

The Deep Q-learning method using the approximation function and neural network is rather complicated1

You have to review and have a deep research in the future!


