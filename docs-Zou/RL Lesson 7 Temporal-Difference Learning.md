## RL Lesson 7: Temporal-Difference Learning

* TD learning often refers to a broad class of RL algorithms
* TD learning also refers to a specific algorithm for estimating state values of a given policy $\pi$.

### 1、TD learning(model free)

The needed data: {$(s_t,r_{t+1},s_{t+1})$} generated following the given policy $\pi$

#### **The algorithm is:**

$$
v_{t+1}(s_t) = v_t(s_t)-\alpha_t(s_t)[v_t(s_t)-[r_{t+1}+\gamma v_t(s_{t+1})]] 
$$

$$
v_{t+1}(s) = v_t(s), \forall s \ne s_t
$$

Attention t+1 and t indicates time sequence!

#### Three things to clarify:

* $v_{t+1}(s_t) and,v_t(s_t)$  are the state values' estimation of the exact state $s_t$, and $\alpha_t(s_t)$ is the coefficient.
* Denote the TD target:

$$
\bar{v_t} \doteq r_{t+1} + \gamma v(s_{t+1})
$$

​	why can we do that?

​	That is because the algorithm drives $v(s_t)$ towards $\bar(v_t)$ 

* What is the interpretation of the TD error?

$$
\delta_t =v_t(s_t)-[r_{t+1}+\gamma v_t(s_{t+1})]
$$

​	It is a difference between two consequent time steps, namely the temporal difference.

​	And it also reflects the deficiency between $v_t$ and $v_{\pi}$ of a certain state.

![TD difference](printscreen/C7/TD_error.png)

#### Other properties

This TD algorithm only estimates the state value of a given policy and can not estimate the action value,etc.

#### What does this TD algorithm do mathematically?

It solves the Bellman Equation of a given policy $\pi$ without a model !

_____

#### The Bellman expectation equation

$$
v_{\pi}(s) = E[R+\gamma v_{\pi}(S')|S=s] , s \in S
$$

It's an important tool to design and analyze TD algorithms.

To solve the Bellman Equation in using the RM algorithm, we define:
$$
g(v(s)) = v(s) - E[R+\gamma v_{\pi}(S')|S=s]
$$
let it be,
$$
g(v(s)) = 0
$$
then we can only obtain the samples r and s' of R and S', the noisy observation of RM we have should be:
$$
\tilde {g(v(s))} = v(s) - [r + \gamma v_{\pi}(s')]
$$
Then, the RM algorithm,

![pic](printscreen/C7/TD_algorithm.png)

_____

The core task in RL is to find the optimal policies. Nearly all the algorithms are based on this task and designed for this.
So Usually we use the policy iteration method, which includes two parts, namely the policy evaluation and policy improvement.

Depart from this principle, we could easily understand what all these algorithms do! 
* TD algorithm provides an equation to iteratively calculate the estimation of state value torwards the true state value under policy $\pi$ . And we all know the policy evaluation is exactly aiming to calculate the state value of current policy $\pi$ (derived from RM algorithm). So TD is used in the policy evaluation part!
* Review: as for policy evaluation part, what we are goint to do in this part? once we got the current state value with current policy,we could upadte the policy!

$$
\pi_{k+1}(s) = arg \max_\pi \sum_a  \pi(a|s) \Bigg(\sum_r p(r|s,a)r+ \gamma \sum_{s^{'}} p(s^{'}|s,a)v_k(s^{'})\Bigg),\quad s \in S
$$

   Then we can clearly know what does Monte Carlo algorithm do? It could solve the policy improvement problem without a specfic model(model free). 
 
 * Then it comes to the Sarsa algorithm? Similarily what does it do? This algorithm estimate the action value and iteratively converges to the real action value under the current given policy. Why doing this? Because it helps in the policy improvement part as we update the q(s,a) value online!!!Thus,undoubtedly,we could combine the **Sarsa** algorithm with the Monte Carlo algorithm to do the policy improvement part!

______
### 2、Q-learning

The Q-learning algorithm is:

$$
q_{t+1}(s_t,a_t) = q_t(s_t,a_t)-\alpha_t(s_t,a_t) \Bigg[q_t(s_t,a_t)-[r_{t+1}+\gamma \max_{a \in A} q_t(s_{t+1},a)] \Bigg], \quad \forall (s,a) \ne (s_t,a_t)
$$

$$
q_{t+1}(s,a)=q_t(s,a), \forall (s,a) \ne (s_t,a_t)
$$

Q-learning is very similar to Sarsa. They are different only in terms of the TD target:
* The TD target in Q-learning is $r_{t+1}+\gamma \max_{a \in A} q_t(s_{t+1},a)$
* The Td TARGET IN Sarsa is $r_{t+1}+\gamma q_t(s_{t+1},a_{t+1})$
____
### 3、On-policy v.s. off-policy

These two are the core concepts in reinforcement learning！

There exist two policies in a TD learning task:
* The behavoir policy is used to generate experience samples(used in the process)
* The target policy is constantly updated toward an optimal policy.($\pi_t$ to $\pi_{t+1}$)

If the behavoir policy is the same as the target policy , then the kind of learning called on-policy, cause we need the policy in the process. Instead, the learning is called off-policy.

One signficant advantage of off-policy learning is that it can search for optimal policies based on the experience samples generated by **any other policies**!

Q-learning is the off-policy algorithm!
____


